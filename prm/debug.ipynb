{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import PrmPublicDataset\n",
    "from model import SparseEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"./data/prm-public/downsampled\").resolve()\n",
    "train_dataset = PrmPublicDataset(data_dir, type=\"train\", nums=1000)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(uid, user_categorical, item_categorical, item_dense), label = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_categorical.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30, 19])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The used value for unknown_value 0 is one of the values already used for encoding the seen categories.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OrdinalEncoder\n\u001b[0;32m----> 3\u001b[0m OrdinalEncoder(handle_unknown\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39muse_encoded_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, unknown_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, encoded_missing_value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49masarray([[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m], [np\u001b[39m.\u001b[39;49mnan, \u001b[39m2\u001b[39;49m]]))\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1517\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[39mfor\u001b[39;00m cardinality \u001b[39min\u001b[39;00m cardinalities:\n\u001b[1;32m   1516\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value \u001b[39m<\u001b[39m cardinality:\n\u001b[0;32m-> 1517\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1518\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe used value for unknown_value \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m is one of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mvalues already used for encoding the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1521\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseen categories.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m             )\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_missing_indices:\n\u001b[1;32m   1525\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m is_scalar_nan(\n\u001b[1;32m   1526\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded_missing_value\n\u001b[1;32m   1527\u001b[0m     ):\n",
      "\u001b[0;31mValueError\u001b[0m: The used value for unknown_value 0 is one of the values already used for encoding the seen categories."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=0, encoded_missing_value=1).fit(np.asarray([[1, 1], [np.nan, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected i < index_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m user_categorical_embedding \u001b[39m=\u001b[39m SparseEmbedding(\n\u001b[1;32m      2\u001b[0m     cardinalities\u001b[39m=\u001b[39mtrain_dataset\u001b[39m.\u001b[39muser_category_cardinalities,\n\u001b[1;32m      3\u001b[0m     embedding_dim\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[39m=\u001b[39m user_categorical_embedding(\n\u001b[1;32m      7\u001b[0m     inputs\u001b[39m=\u001b[39;49muser_categorical,\n\u001b[1;32m      8\u001b[0m     feature_names\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49muser_categorical_feature_names,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/model.py:48\u001b[0m, in \u001b[0;36mSparseEmbedding.forward\u001b[0;34m(self, inputs, feature_names)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m idx, feature_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(feature_names):\n\u001b[1;32m     47\u001b[0m     input_ \u001b[39m=\u001b[39m inputs[:, idx]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategory_embeddings[feature_name](input_)\n\u001b[1;32m     49\u001b[0m     outputs[feature_name] \u001b[39m=\u001b[39m output\n\u001b[1;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:389\u001b[0m, in \u001b[0;36mEmbeddingBag.forward\u001b[0;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, offsets: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, per_sample_weights: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass of EmbeddingBag.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m          returned vectors filled by zeros.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding_bag(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, offsets,\n\u001b[1;32m    390\u001b[0m                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type,\n\u001b[1;32m    391\u001b[0m                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse,\n\u001b[1;32m    392\u001b[0m                            per_sample_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minclude_last_offset,\n\u001b[1;32m    393\u001b[0m                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx)\n",
      "File \u001b[0;32m~/Documents/projects/recsys/ltr/prm/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2392\u001b[0m, in \u001b[0;36membedding_bag\u001b[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001b[0m\n\u001b[1;32m   2385\u001b[0m \u001b[39mif\u001b[39;00m per_sample_weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2386\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   2387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39membedding_bag: per_sample_weights was not None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mper_sample_weights is only supported for mode=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2389\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(got mode=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m). Please open a feature request on GitHub.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mode)\n\u001b[1;32m   2390\u001b[0m     )\n\u001b[0;32m-> 2392\u001b[0m ret, _, _, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49membedding_bag(\n\u001b[1;32m   2393\u001b[0m     weight, \u001b[39minput\u001b[39;49m, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx\n\u001b[1;32m   2394\u001b[0m )\n\u001b[1;32m   2395\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected i < index_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "user_categorical_embedding = SparseEmbedding(\n",
    "    cardinalities=train_dataset.user_category_cardinalities,\n",
    "    embedding_dim=8,\n",
    ")\n",
    "\n",
    "outputs = user_categorical_embedding(\n",
    "    inputs=user_categorical,\n",
    "    feature_names=train_dataset.user_categorical_feature_names,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
